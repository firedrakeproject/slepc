%-------------------------------------------------------
% SLEPc Users Manual
%-------------------------------------------------------
\chapter{\label{cap:pep}PEP: Polynomial Eigenvalue Problems}
%-------------------------------------------------------

%\begin{center}
%  {\setlength{\fboxsep}{4mm}
%  \framebox{%
%   \begin{minipage}{.8\textwidth}
%   \textbf{Note:} Previous \slepc versions provided specific functionality for quadratic eigenvalue problems (the \texttt{QEP} module). This has been made more general by covering polynomials of arbitrary degree (the \texttt{PEP} module described in this chapter), which of course includes quadratic problems as a particular case. See \S\ref{sec:qeppep} for how to upgrade application code that used \texttt{QEP}.
%   \end{minipage}
%  }}
%\end{center}

\noindent The Polynomial Eigenvalue Problem (\ident{PEP}) solver object is intended for addressing polynomial eigenproblems of arbitrary degree, $P(\lambda)x=0$. A particular instance is the quadratic eigenvalue problem (degree 2), which is the case more often encountered in practice. For this reason, part of the description of this chapter focuses specifically on quadratic eigenproblems.

Currently, most \ident{PEP} solvers are based on linearization, either implicit or explicit. The case of explicit linearization allows the use of eigensolvers from \ident{EPS} to solve the linearized problem.

\section{\label{sec:pep}Overview of Polynomial Eigenproblems}

In this section, we review some basic properties of the polynomial eigenvalue problem. The main goal is to set up the notation as well as to describe the linearization approaches that will be employed for solving via an \ident{EPS} object.
To simplify the description, we initially restrict to the case of quadratic eigenproblems, and then extend to the general case of arbitrary degree.
For additional background material, the reader is referred to \citep{Tisseur:2001:QEP}. More information can be found in a paper by \cite{Campos:2016:PKS}, that focuses specifically on \slepc implementation of methods based on Krylov iterations on the linearized problem.

\subsection{\label{sec:qep}Quadratic Eigenvalue Problems}

In many applications, e.g., problems arising from second-order differential equations such as the analysis of damped vibrating systems, the eigenproblem to be solved is quadratic,
\begin{equation}
(K+\lambda C+\lambda^2M)x=0,\label{eq:eigquad}
\end{equation}
where $K,C,M\in\mathbb{C}^{n\times n}$ are the coefficients of a matrix polynomial of degree 2, $\lambda\in\mathbb{C}$ is the eigenvalue and $x\in\mathbb{C}^n$ is the eigenvector. As in the case of linear eigenproblems, the eigenvalues and eigenvectors can be complex even in the case that all three matrices are real.

It is important to point out some outstanding differences with respect to the linear eigenproblem. In the quadratic eigenproblem, the number of eigenvalues is $2n$, and the corresponding eigenvectors do not form a linearly independent set. If $M$ is singular, some eigenvalues are infinite. Even when the three matrices are symmetric and positive definite, there is no guarantee that the eigenvalues are real, but still methods can exploit symmetry to some extent. Furthermore, numerical difficulties are more likely than in the linear case, so the computed solution can sometimes be untrustworthy.

If Eq.\ \ref{eq:eigquad} is written as $P(\lambda)x=0$, where $P$ is the matrix polynomial, then multiplication by $\lambda^{-2}$ results in $R(\lambda^{-1})x=0$, where $R$ is a matrix polynomial with the coefficients in the reverse order. In other words, if a method is available for computing the largest eigenvalues, then reversing the roles of $M$ and $K$ results in the computation of the smallest eigenvalues. In general, it is also possible to formulate a spectral transformation for computing eigenvalues closest to a given target, as discussed in \S\ref{sec:qst}.

\paragraph{Problem Types.}

As in the case of linear eigenproblems, there are some particular properties of the coefficient matrices that confer a certain structure to the quadratic eigenproblem, e.g., symmetry of the spectrum with respect to the real or imaginary axes. These structures are important as long as the solvers are able to exploit them.

\begin{itemize}
\item Hermitian (symmetric) problems, when $M$, $C$, $K$ are all Hermitian (symmetric). Eigenvalues are real or come in complex conjugate pairs. Furthermore, if $M>0$ and $C,K\geq 0$ then the system is stable, i.e., $\text{Re}(\lambda)\leq 0$.
\item Hyperbolic problems, a particular class of Hermitian problems where $M>0$ and $(x^*Cx)^2>4(x^*Mx)(x^*Kx)$ for all nonzero $x\in\mathbb{C}^n$. All eigenvalues are real, and form two separate groups of $n$ eigenvalues, each of them having linearly independent eigenvectors.
\item Overdamped problems, a specialized type of hyperbolic problems, where $C>0$ and $K\geq 0$. The eigenvalues are non-positive.
\item Gyroscopic problems, when $M$, $K$ are Hermitian, $M>0$, and $C$ is skew-Hermitian, $C=-C^*$. The spectrum is symmetric with respect to the imaginary axis, and in the real case, it has a Hamiltonian structure, i.e., eigenvalues come in quadruples $(\lambda,\bar{\lambda},-\lambda,-\bar{\lambda})$.
\end{itemize}

Currently, the problem type is not exploited by \ident{PEP} solvers, except for a few exceptions. In the future, we may add more support for structure-preserving solvers.

\paragraph{Linearization.}

It is possible to transform the quadratic eigenvalue problem to a linear generalized eigenproblem $L_0y=\lambda L_1y$ by doubling the order of the system, i.e., $L_0,L_1\in\mathbb{C}^{2n\times 2n}$. There are many ways of doing this.
Below, we show some of the most common linearizations,
which are based on defining the eigenvector of the linear problem as
\begin{equation}
\label{eq:linevec}
y=\left[\begin{array}{c}x\\x\lambda\end{array}\right].
\end{equation}

\begin{itemize}
\item Non-symmetric linearizations. The resulting matrix pencil has no particular structure.
\begin{equation}
\label{eq:n1}
\mbox{N1:}\qquad
\left[\begin{array}{cc}0 & I\\-K & -C\end{array}\right]-\lambda\left[\begin{array}{cc}I & 0\\0 & M\end{array}\right]
\end{equation}
\begin{equation}
\label{eq:n2}
\mbox{N2:}\qquad
\left[\begin{array}{cc}-K & 0\\0 & I\end{array}\right]-\lambda\left[\begin{array}{cc}C & M\\I & 0\end{array}\right]
\end{equation}

\medskip
\item Symmetric linearizations. If $M$, $C$, and $K$ are all symmetric (Hermitian), the resulting matrix pencil is symmetric (Hermitian), although indefinite.
\begin{equation}
\label{eq:s1}
\mbox{S1:}\qquad
\left[\begin{array}{cc}0 & -K\\-K & -C\end{array}\right]-\lambda\left[\begin{array}{cc}-K & 0\\0 & M\end{array}\right]
\end{equation}
\begin{equation}
\label{eq:s2}
\mbox{S2:}\qquad
\left[\begin{array}{cc}-K & 0\\0 & M\end{array}\right]-\lambda\left[\begin{array}{cc}C & M\\ M & 0\end{array}\right]
\end{equation}

\medskip
\item Hamiltonian linearizations. If the quadratic eigenproblem is gyroscopic, one of the matrices is Hamiltonian and the other is skew-Hamiltonian. The first form (H1) is recommended when $M$ is singular, whereas the second form (H2) is recommended when $K$ is singular.
\begin{equation}
\label{eq:h1}
\mbox{H1:}\qquad
\left[\begin{array}{cc}K & 0\\C & K\end{array}\right]-\lambda\left[\begin{array}{cc} 0 & K\\-M & 0\end{array}\right]
\end{equation}
\begin{equation}
\label{eq:h2}
\mbox{H2:}\qquad
\left[\begin{array}{cc}0 & -K\\M & 0\end{array}\right]-\lambda\left[\begin{array}{cc}M & C\\ 0 & M\end{array}\right]
\end{equation}
\end{itemize}

In \slepc, the \ident{PEPLINEAR} solver is based on using one of the above linearizations for solving the quadratic eigenproblem. This solver makes use of linear eigensolvers from the \ident{EPS} package.

We could also consider the \emph{reversed} forms, e.g., the reversed form of N2 is
\begin{equation}
\label{eq:n2r}
\mbox{N2-R:}\qquad
\left[\begin{array}{cc}-C & -M\\I & 0\end{array}\right]-\frac{1}{\lambda}\left[\begin{array}{cc}K & 0\\0 & I\end{array}\right],
\end{equation}
which is equivalent to the form N1 for the problem $R(\lambda^{-1})x=0$. These reversed forms are not implemented in \slepc, but the user can use them simply by reversing the roles of $M$ and $K$, and considering the reciprocals of the computed eigenvalues. Alternatively, this can be viewed as a particular case of the spectral transformation (with $\sigma=0$), see \S\ref{sec:qst}.

\subsection{\label{sec:pep1}Polynomials of Arbitrary Degree}

In general, the polynomial eigenvalue problem can be formulated as
\begin{equation}
\label{eq:pep}
P(\lambda)x=0,
\end{equation}
where $P$ is an $n\times n$ matrix polynomial of degree $d$. An $n$-vector $x\neq 0$ satisfying this equation is called an eigenvector associated with the corresponding eigenvalue $\lambda$.

We start by considering the case where $P$ is expressed in terms of the monomial basis,
\begin{equation}
\label{eq:pepmon}
P(\lambda)=A_0+A_1 \lambda+A_2\lambda^2 +  \dotsb + A_d \lambda^d,
\end{equation}
where $A_0,\ldots,A_d$ are the $n\times n$ coefficient matrices. As before, the problem can be solved via some kind of linearization. One of the most commonly used one is the first companion form
\begin{equation}
\label{eq:firstcomp}
L(\lambda)=L_0 -\lambda L_1,
\end{equation}
where the related linear eigenproblem is $L(\lambda)y=0$, with
\begin{equation}
\label{eq:firstcompfull}
L_0 =
\begin{bmatrix}
  & I \\
  & & \ddots \\
  & & & I \\
  -A_0 & -A_1 & \cdots  & -A_{d-1}
\end{bmatrix},\quad
L_1 =
\begin{bmatrix}
  I \\
  & \ddots \\
  & & I \\
  & & & A_d
\end{bmatrix}, \quad
y=
\begin{bmatrix}
  x \\ x\lambda\\ \vdots \\ x\lambda^{d-1}
\end{bmatrix}.
\end{equation}
This is the generalization of Eq.\ \ref{eq:n1}.

The definition of vector $y$ above contains the successive powers of $\lambda$. For large polynomial degree, these values may produce overflow in finite precision computations, or at least lead to numerical instability of the algorithms due to the wide difference in magnitude of the eigenvector entries. For this reason, it is generally recommended to work with non-monomial polynomial bases whenever the degree is not small, e.g., for $d>5$.

In the most general formulation of the polynomial eigenvalue problem, $P$ is expressed as
\begin{equation}
\label{eq:pepnonmon}
P(\lambda)=A_0\phi_0(\lambda)+A_1\phi_1(\lambda)+\dots+A_d\phi_d(\lambda),
\end{equation}
where $\phi_i$ are the members of a given polynomial basis, for instance, some kind of orthogonal polynomials such as Chebyshev polynomials of the first kind. In that case, the expression of $y$ in Eq.\ \ref{eq:firstcompfull} contains $\phi_0(\lambda),\dots,\phi_d(\lambda)$ instead of the powers of $\lambda$. Correspondingly, the form of $L_0$ and $L_1$ is different for each type of polynomial basis.

\paragraph{Avoiding the Linearization.}

An alternative to linearization is to directly perform a projection of the polynomial eigenproblem. These methods enforce a Galerkin condition on the polynomial residual, $P(\theta)u\perp \mathcal{K}$. Here, the subspace $\mathcal{K}$ can be built in various ways, for instance with the Jacobi-Davidson method. This family of methods need not worry about operating with vectors of dimension $dn$. The downside is that computing more than one eigenvalue is more difficult, since usual deflation strategies cannot be applied.

%---------------------------------------------------
\section{Basic Usage}

The user interface of the \ident{PEP} package is very similar to \ident{EPS}. For basic usage, the most noteworthy difference is that all coefficient matrices $A_i$ have to be supplied in the form of an array of \texttt{Mat}.

A basic example code for solving a polynomial eigenproblem with \ident{PEP} is shown in Figure \ref{fig:ex-pep}, where the code for building matrices \texttt{A[0]}, \texttt{A[1]}, \ldots\ is omitted. The required steps are the same as those described in chapter \ref{cap:eps} for the linear eigenproblem. As always, the solver context is created with \ident{PEPCreate}. The coefficient matrices are provided with \ident{PEPSetOperators}, and the problem type is specified with \ident{PEPSetProblemType}. Calling \ident{PEPSetFromOptions} allows the user to set up various options through the command line. The call to \ident{PEPSolve} invokes the actual solver. Then, the solution is retrieved with \ident{PEPGetConverged} and \ident{PEPGetEigenpair}. Finally, \ident{PEPDestroy} destroys the object.

\begin{figure}
\begin{Verbatim}[fontsize=\small,numbers=left,numbersep=6pt,xleftmargin=15mm]
#define NMAT 5
PEP         pep;       /*  eigensolver context  */
Mat         A[NMAT];   /*  coefficient matrices */
Vec         xr, xi;    /*  eigenvector, x       */
PetscScalar kr, ki;    /*  eigenvalue, k        */
PetscInt    j, nconv;
PetscReal   error;

PEPCreate( PETSC_COMM_WORLD, &pep );
PEPSetOperators( pep, NMAT, A );
PEPSetProblemType( pep, PEP_GENERAL );  /* optional */
PEPSetFromOptions( pep );
PEPSolve( pep );
PEPGetConverged( pep, &nconv );
for (j=0; j<nconv; j++) {
  PEPGetEigenpair( pep, j, &kr, &ki, xr, xi );
  PEPComputeError( pep, j, PEP_ERROR_BACKWARD, &error );
}
PEPDestroy( &pep );
\end{Verbatim}
\caption{\label{fig:ex-pep}Example code for basic solution with \ident{PEP}.}
\end{figure}


%---------------------------------------------------
\section{Defining the Problem}

\begin{table}[t]
\centering
{\small \begin{tabular}{lll}
                   &                      & {\footnotesize Options} \\
Polynomial Basis     & \ident{PEPBasis}                & {\footnotesize Database Name}\\\hline
Monomial             & \texttt{PEP\_BASIS\_MONOMIAL}   & \texttt{monomial}\\
Chebyshev (1st kind) & \texttt{PEP\_BASIS\_CHEBYSHEV1} & \texttt{chebyshev1}\\
Chebyshev (2nd kind) & \texttt{PEP\_BASIS\_CHEBYSHEV2} & \texttt{chebyshev2}\\
Legendre             & \texttt{PEP\_BASIS\_LEGENDRE}   & \texttt{legendre}\\
Laguerre             & \texttt{PEP\_BASIS\_LAGUERRE}   & \texttt{laguerre}\\
Hermite              & \texttt{PEP\_BASIS\_HERMITE}    & \texttt{hermite}\\\hline
\end{tabular} }
\caption{\label{tab:pepbasis}Polynomial bases available to represent the matrix polynomial in \ident{PEP}.}
\end{table}

As explained in \S\ref{sec:pep1}, the matrix polynomial $P(\lambda)$ can be expressed in term of the monomials $1$, $\lambda$, $\lambda^2,\ldots$, or in a non-monomial basis as in Eq.\ \ref{eq:pepnonmon}. Hence, when defining the problem we must indicate which is the polynomial basis to be used as well as the coefficient matrices $A_i$ in that basis representation. By default, a monomial basis is used. Other possible bases are listed in Table \ref{tab:pepbasis}, and can be set with
	\findex{PEPSetBasis}
	\begin{Verbatim}[fontsize=\small]
	PEPSetBasis(PEP pep,PEPBasis basis);
	\end{Verbatim}
or with the command-line key \Verb!-pep_basis <name>!. The matrices are passed with
	\findex{PEPSetOperators}
	\begin{Verbatim}[fontsize=\small]
	PEPSetOperators(PEP pep,PetscInt nmat,Mat A[]);
	\end{Verbatim}

\begin{table}[b]
\centering
{\small \begin{tabular}{lll}
Problem Type  & \ident{PEPProblemType}    & Command line key\\\hline
General       & \texttt{PEP\_GENERAL}     & \texttt{-pep\_general}\\
Hermitian     & \texttt{PEP\_HERMITIAN}   & \texttt{-pep\_hermitian}\\
Gyroscopic    & \texttt{PEP\_GYROSCOPIC}  & \texttt{-pep\_gyroscopic}\\\hline
\end{tabular} }
\caption{\label{tab:ptypeq}Problem types considered in \ident{PEP}.}
\end{table}

As mentioned in \S\ref{sec:qep}, it is possible to distinguish among different problem types. The problem types currently supported for \ident{PEP} are listed in Table \ref{tab:ptypeq}. The goal when choosing an appropriate problem type is to let the solver exploit the underlying structure, in order to possibly compute the solution more accurately with less floating-point operations. When in doubt, use the default problem type (\texttt{PEP\_GENERAL}).

The problem type can be specified at run time with the corresponding command line key or, more usually, within the program with the function
	\findex{PEPSetProblemType}
	\begin{Verbatim}[fontsize=\small]
	EPSSetProblemType(PEP eps,PEPProblemType type);
	\end{Verbatim}
Currently, the problem type is ignored in most solvers and it is taken into account only in some cases for the quadratic eigenproblem only.

Apart from the polynomial basis and the problem type, the definition of the problem is completed with the number and location of the eigenvalues to compute. This is done very much like in \ident{EPS}, but with minor differences.

The number of eigenvalues (and eigenvectors) to compute, \texttt{nev}, is specified with the function%
	\findex{PEPSetDimensions}
	\begin{Verbatim}[fontsize=\small]
	PEPSetDimensions(PEP pep,PetscInt nev,PetscInt ncv,PetscInt mpd);
	\end{Verbatim}
The default is to compute only one. This function also allows control over the dimension of the subspaces used internally. The second argument, \texttt{ncv}, is the number of column vectors to be used by the solution algorithm, that is, the largest dimension of the working subspace. The third argument, \texttt{mpd}, is the maximum projected dimension. These parameters can also be set from the command line with \Verb!-pep_nev!, \Verb!-pep_ncv! and \Verb!-pep_mpd!.

For the selection of the portion of the spectrum of interest, there are several alternatives listed in Table~\ref{tab:portionq}, to be selected with the function
	\findex{PEPSetWhichEigenpairs}
	\begin{Verbatim}[fontsize=\small]
	PEPSetWhichEigenpairs(PEP pep,PEPWhich which);
	\end{Verbatim}
The default is to compute the largest magnitude eigenvalues.
For the sorting criteria relative to a target value, the scalar $\tau$ must be specified with:
	\findex{PEPSetTarget}
	\begin{Verbatim}[fontsize=\small]
	PEPSetTarget(PEP pep,PetscScalar target);
	\end{Verbatim}
or in the command-line with \Verb!-pep_target!. As in \ident{EPS}, complex values of $\tau$ are allowed only in complex scalar SLEPc builds. The criteria relative to a target must be used in combination with a spectral transformation as explained in \S\ref{sec:qst}.

Finally, we mention that the use of regions for filtering is also available in \ident{PEP}, see \S\ref{sec:region}.

\begin{table}
\centering
{\small \begin{tabular}{lll}
\texttt{PEPWhich}                  & Command line key                   & Sorting criterion \\\hline
\texttt{PEP\_LARGEST\_MAGNITUDE}   & \texttt{-pep\_largest\_magnitude}  & Largest $|\lambda|$ \\
\texttt{PEP\_SMALLEST\_MAGNITUDE}  & \texttt{-pep\_smallest\_magnitude} & Smallest $|\lambda|$ \\
\texttt{PEP\_LARGEST\_REAL}        & \texttt{-pep\_largest\_real}       & Largest $\mathrm{Re}(\lambda)$ \\
\texttt{PEP\_SMALLEST\_REAL}       & \texttt{-pep\_smallest\_real}      & Smallest $\mathrm{Re}(\lambda)$ \\
\texttt{PEP\_LARGEST\_IMAGINARY}   & \texttt{-pep\_largest\_imaginary}  & Largest $\mathrm{Im}(\lambda)$\footnotemark \\
\texttt{PEP\_SMALLEST\_IMAGINARY}  & \texttt{-pep\_smallest\_imaginary} & Smallest $\mathrm{Im}(\lambda)$\addtocounter{footnote}{-1}\footnotemark \\\hline
\texttt{PEP\_TARGET\_MAGNITUDE}    & \texttt{-pep\_target\_magnitude}   & Smallest $|\lambda-\tau|$ \\
\texttt{PEP\_TARGET\_REAL}         & \texttt{-pep\_target\_real}        & Smallest $|\mathrm{Re}(\lambda-\tau)|$ \\
\texttt{PEP\_TARGET\_IMAGINARY}    & \texttt{-pep\_target\_imaginary}   & Smallest $|\mathrm{Im}(\lambda-\tau)|$ \\\hline
\texttt{PEP\_WHICH\_USER}          &                                    & \emph{user-defined} \\\hline
\end{tabular} }
\caption{\label{tab:portionq}Available possibilities for selection of the eigenvalues of interest in \ident{PEP}.}
\end{table}

\footnotetext{If \slepc is compiled for real scalars, then the absolute value of the imaginary part, $|\mathrm{Im}(\lambda)|$, is used for eigenvalue selection and sorting.}

%---------------------------------------------------
\section{Selecting the Solver}

The solution method can be specified procedurally with
	\findex{PEPSetType}
	\begin{Verbatim}[fontsize=\small]
	PEPSetType(PEP pep,PEPType method);
	\end{Verbatim}
or via the options database command \Verb!-pep_type! followed by the name of the method. The methods currently available in \ident{PEP} are listed in Table~\ref{tab:solversp}. The solvers in the first group are based on the linearization explained above, whereas solvers in the second group perform a projection on the polynomial problem (without linearizing).

The default solver is \ident{PEPTOAR}. TOAR is a stable algorithm for building an Arnoldi factorization of the linearization (\ref{eq:firstcomp}) without explicitly creating matrices $L_0,L_1$, and represents the Krylov basis in a compact way. STOAR is a variant of TOAR that exploits symmetry (requires \texttt{PEP\_HERMITIAN} problem type). Q-Arnoldi is related to TOAR and follows a similar approach.

\begin{table}
\centering
{\small \begin{tabular}{lllll}
                   &                   & {\footnotesize Options} & {\footnotesize Polynomial} & {\footnotesize Polynomial} \\
Method             & \ident{PEPType}   & {\footnotesize Database Name} & {\footnotesize Degree} & {\footnotesize Basis} \\\hline
Two-level Orthogonal Arnoldi (TOAR) & \texttt{PEPTOAR}     & \texttt{toar}     & Arbitrary & Any \\
Symmetric TOAR                      & \texttt{PEPSTOAR}    & \texttt{stoar}    & Quadratic & Monomial \\
Quadratic Arnoldi (Q-Arnoldi)       & \texttt{PEPQARNOLDI} & \texttt{qarnoldi} & Quadratic & Monomial \\
Linearization via \ident{EPS}       & \texttt{PEPLINEAR}   & \texttt{linear}   & Arbitrary & Any\footnotemark \\
\hline
Jacobi-Davidson                     & \texttt{PEPJD}       & \texttt{jd}       & Arbitrary & Monomial \\
\hline
\end{tabular} }
\caption{\label{tab:solversp}Polynomial eigenvalue solvers available in the \ident{PEP} module.}
\end{table}
\footnotetext{Except if the explicit matrix option is set.}

The \ident{PEPLINEAR} method carries out an explicit linearization of the quadratic eigenproblem, as described in \S\ref{sec:qep}, resulting in a generalized eigenvalue problem that is handled by an \ident{EPS} object created internally. If required, this \ident{EPS} object can be extracted with the operation
	\findex{PEPLinearGetEPS}
	\begin{Verbatim}[fontsize=\small]
	PEPLinearGetEPS(PEP pep,EPS *eps);
	\end{Verbatim}
This allows the application programmer to set any of the \ident{EPS} options directly within the code. Also, it is possible to change the \ident{EPS} options through the command-line, simply by prefixing the \ident{EPS} options with \texttt{-pep\_}.

In \ident{PEPLINEAR}, the expression used in the linearization is specified by two parameters:
\begin{enumerate}
\item The problem type set with \ident{PEPProblemType}, which chooses from non-symmetric, symmetric, and Hamiltonian linearizations.
\item The companion form, 1 or 2, that can be chosen with
	\findex{PEPLinearSetCompanionForm}
	\begin{Verbatim}[fontsize=\small]
   PEPLinearSetCompanionForm(PEP pep,PetscInt cform);
	\end{Verbatim}
\end{enumerate}

Another option of the \ident{PEPLINEAR} solver is whether the matrices of the linearized problem are created explicitly or not. This is set with the function
	\findex{PEPLinearSetExplicitMatrix}
	\begin{Verbatim}[fontsize=\small]
	PEPLinearSetExplicitMatrix(PEP pep,PetscBool exp);
	\end{Verbatim}
The explicit matrix option is available only for quadratic eigenproblems (higher degree polynomials are always handled implicitly). In the case of explicit creation, matrices $L_0$ and $L_1$ are created as true \texttt{Mat}'s, with explicit storage, whereas the implicit option works with \emph{shell} \texttt{Mat}'s that operate only with the constituent blocks $M$, $C$ and $K$ (or $A_i$ in the general case). The explicit case requires more memory but gives more flexibility, e.g., for choosing a preconditioner. Some examples of usage via the command line are shown at the end of next section.

%---------------------------------------------------
\section{\label{sec:qst}Spectral Transformation}

For computing eigenvalues in the interior of the spectrum (closest to a target $\tau$), it is necessary to use a spectral transformation. In \ident{PEP} solvers this is handled via an \ident{ST} object as in the case of linear eigensolvers. It is possible to proceed with no spectral transformation (shift) or with shift-and-invert. Every \ident{PEP} object has an \ident{ST} object internally.
%Note that it would also be possible to select a spectral transformation in the \ident{ST} contained in the \ident{EPS} solver for the \texttt{PEPLINEAR} case. However, we do not recommend this approach.

The spectral transformation can be applied either to the polynomial problem or its linearization. We illustrate it first for the quadratic case.

Given the quadratic eigenproblem in Eq.\ \ref{eq:eigquad}, it is possible to define the transformed problem
\begin{equation}
\label{eq:sinvquad}
(K_\sigma+\theta C_\sigma+\theta^2M_\sigma)x=0,
\end{equation}
where the coefficient matrices are
\begin{eqnarray}
K_\sigma&\!\!=\!\!&M,\\
C_\sigma&\!\!=\!\!&C+2\sigma M,\\
M_\sigma&\!\!=\!\!&\sigma^2 M+\sigma C+K,
\end{eqnarray}
and the relation between the eigenvalue of the original eigenproblem, $\lambda$, and the transformed one, $\theta$, is $\theta=(\lambda-\sigma)^{-1}$ as in the case of the linear eigenvalue problem. See chapter \ref{cap:st} for additional details.

The polynomial eigenvalue problem of Eq.\ \ref{eq:sinvquad} corresponds to the reversed form of the shifted polynomial, $R(\theta)$. The extension to matrix polynomials of arbitrary degree is also possible, where the coefficients of $R(\theta)$ have the general form
\begin{equation}
\label{eq:sinvpep}
T_k=\sum_{j=0}^{d-k}\binom{j+k}{k}\sigma^{j}A_{j+k},\qquad k=0,\ldots,d.
\end{equation}
The way this is implemented in \slepc is that the \ident{ST} object is in charge of computing the $T_k$ matrices, so that the \ident{PEP} solver operates with these matrices as it would with the original $A_i$ matrices, without changing its behaviour. We say that \ident{ST} performs the transformation.

An alternative would be to apply the shift-and-invert spectral transformation to the linearization (Eq.\ \ref{eq:firstcomp}) in a smart way, making the polynomial eigensolver aware of this fact so that it can exploit the block structure of the linearization.
Let $S_\sigma:=(L_0-\sigma L_1)^{-1}L_1$, then when the solver needs to extend the Arnoldi basis with an operation such as $z=S_\sigma w$, a linear solve is required with the form
\begin{equation}
\label{eq:sinvpeplin}
\begin{bmatrix}
  -\sigma I  & I \\
  & -\sigma I & \ddots \\
  & & \ddots & I \\
  & & & -\sigma I & I \\
  -A_0 & -A_1 & \cdots  & -\tilde{A}_{d-2} & -\tilde{A}_{d-1}
\end{bmatrix}
\begin{bmatrix}
  z^0\\z^1\\\vdots\\z^{d-2}\\z^{d-1}
\end{bmatrix}
  =
\begin{bmatrix}
  w^0\\w^1\\\vdots\\w^{d-2}\\A_dw^{d-1}
\end{bmatrix},
\end{equation}
with $\tilde{A}_{d-2}=A_{d-2}+\sigma I$ and $\tilde{A}_{d-1}=A_{d-1}+\sigma A_d$.
From the block LU factorization, it is possible to derive a simple recurrence to compute $z^i$, with one of the steps involving a linear solve with $P(\sigma)$.

Implementing the latter approach is more difficult (especially if different polynomial bases must be supported), and requires an intimate relation with the \ident{PEP} solver. That is why it is only available currently in the default solver (TOAR) and in \texttt{PEPLINEAR} without explicit matrix. In order to choose between the two approaches, the user can set a flag with
	\findex{STSetTransform}
	\begin{Verbatim}[fontsize=\small]
	STSetTransform(ST st,PetscBool flg);
	\end{Verbatim}
(or in the command line \Verb!-st_transform!) to activate the first one (\ident{ST} performs the transformation). Note that this flag belongs to \ident{ST}, not \ident{PEP} (use \ident{PEPGetST} to extract it).

In terms of overall computational cost, both approaches are roughly equivalent, but the advantage of the second one is not having to store the $T_k$ matrices explicitly. It may also be slightly more accurate. Hence, the \ident{STSetTransform} flag is turned off by default. Please note that using shift-and-invert with solvers other than TOAR may require turning it on explicitly.

A command line example would be:
	\begin{Verbatim}[fontsize=\small]
	$ ./ex16 -pep_nev 12 -pep_type toar -pep_target 0 -st_type sinvert
	\end{Verbatim}
The example computes 12 eigenpairs closest to the origin with TOAR and shift-and-invert. The \Verb!-st_transform! could be added optionally to switch to \ident{ST} being in charge of the transformation. The same example with Q-Arnoldi would be
	\begin{Verbatim}[fontsize=\small]
	$ ./ex16 -pep_nev 12 -pep_type qarnoldi -pep_target 0 -st_type sinvert
                 -st_transform
	\end{Verbatim}
where in this case \Verb!-st_transform! would be set as default if not specified.

As a complete example of how to solve a quadratic eigenproblem via explicit linearization with explicit construction of the $L_0$ and $L_1$ matrices, consider the following command line:
\begin{Verbatim}[fontsize=\small]
	$ ./sleeper -pep_type linear -pep_target -10 -pep_linear_st_type sinvert
                    -pep_linear_st_ksp_type preonly -pep_linear_st_pc_type lu
                    -pep_linear_st_pc_factor_mat_solver_package mumps
                    -mat_mumps_icntl_14 100 -pep_linear_explicitmatrix
\end{Verbatim}
This example uses MUMPS for solving the associated linear systems, see \S\ref{sec:lin} for details. The following command line example illustrates how to solve the same problem without explicitly forming the matrices. Note that in this case the \ident{ST} options are not prefixed with \texttt{-pep\_linear\_} since now they do not refer to the \ident{ST} within the \ident{PEPLINEAR} solver but the general \ident{ST} associated to \ident{PEP}.
\begin{Verbatim}[fontsize=\small]
	$ ./sleeper -pep_type linear -pep_target -10 -st_type sinvert
                    -st_ksp_type preonly -st_pc_type lu
                    -st_pc_factor_mat_solver_package mumps -mat_mumps_icntl_14 100
\end{Verbatim}

%---------------------------------------------------
\section{Retrieving the Solution}

After the call to \ident{PEPSolve} has finished, the computed results are stored internally. The procedure for retrieving the computed solution is exactly the same as in the case of \ident{EPS}. The user has to call \ident{PEPGetConverged} first, to obtain the number of converged solutions, then call \ident{PEPGetEigenpair} repeatedly within a loop, once per each eigenvalue-eigenvector pair. The same considerations relative to complex eigenvalues apply, see \S\ref{sec:retrsol} for additional details.

\paragraph{Reliability of the Computed Solution.}

\begin{table}
\centering
{\small \begin{tabular}{llll}
Error type     & \texttt{PEPErrorType}         & Command line key          & Error bound \\\hline
Absolute error & \texttt{PEP\_ERROR\_ABSOLUTE} & \texttt{-pep\_error\_absolute} & $\|r\|$ \\
Relative error & \texttt{PEP\_ERROR\_RELATIVE} & \texttt{-pep\_error\_relative} & $\|r\|/|\lambda|$ \\
Backward error & \texttt{PEP\_ERROR\_BACKWARD} & \texttt{-pep\_error\_backward} & $\|r\|/(\sum_j\|A_j\||\lambda_i|^j)$ \\
\hline
\end{tabular} }
\caption{\label{tab:peperrors}Possible expressions for computing error bounds.}
\end{table}

As in the case of linear problems, the function
	\findex{PEPComputeError}
	\begin{Verbatim}[fontsize=\small]
	PEPComputeError(PEP pep,PetscInt j,PEPErrorType type,PetscReal *error);
	\end{Verbatim}
is available to assess the accuracy of the computed solutions. This error is based on the computation of the 2-norm of the residual vector, defined as
\begin{equation}
r=P(\tilde{\lambda})\tilde{x},\label{eq:respol}
\end{equation}
where $\tilde{\lambda}$ and $\tilde{x}$ represent any of the \texttt{nconv} computed eigenpairs delivered by \ident{PEPGetEigenpair}.
From the residual norm, the error bound can be computed in different ways, see Table \ref{tab:peperrors}. It is usually recommended to assess the accuracy of the solution using the backward error, defined as
\begin{equation}
\eta(\tilde{\lambda},\tilde{x})=\frac{\|r\|}{\sum_{j=0}^d\|A_j\||\tilde\lambda|^j\|\tilde{x}\|},\label{eq:backward}
\end{equation}
where $d$ is the degree of the polynomial. Note that the eigenvector is always assumed to have unit norm.

Similar expressions can be used in the convergence criterion used to accept converged eigenpairs internally by the solver. The convergence test can be set via the corresponding command-line switch (see Table \ref{tab:pepconv}) or with
	\findex{PEPSetConvergenceTest}
	\begin{Verbatim}[fontsize=\small]
	PEPSetConvergenceTest(PEP pep,PEPConv conv);
	\end{Verbatim}

\begin{table}
\centering
{\small \begin{tabular}{llll}
Convergence criterion    & \texttt{PEPConv}         & Command line key          & Error bound \\\hline
Absolute                 & \texttt{PEP\_CONV\_ABS}  & \texttt{-pep\_conv\_abs}  & $\|r\|$ \\
Relative to eigenvalue   & \texttt{PEP\_CONV\_REL}  & \texttt{-pep\_conv\_rel}  & $\|r\|/|\lambda|$ \\
Relative to matrix norms & \texttt{PEP\_CONV\_NORM} & \texttt{-pep\_conv\_norm} & $\|r\|/(\sum_j\|A_j\||\lambda_i|^j)$ \\
User-defined             & \texttt{PEP\_CONV\_USER} & \texttt{-pep\_conv\_user} & user function \\
\hline
\end{tabular} }
\caption{\label{tab:pepconv}Available possibilities for the convergence criterion.}
\end{table}

\paragraph{Scaling.}

When solving a quadratic eigenproblem via linearization, an accurate solution of the generalized eigenproblem does not necessarily imply a similar level of accuracy for the quadratic problem. \cite{Tisseur:2000:BEC} shows that in the case of the N1 linearization (Eq.\ \ref{eq:n1}), a small backward error in the generalized eigenproblem guarantees a small backward error in the quadratic eigenproblem. However, this holds only if $M$, $C$ and $K$ have a similar norm.

When the norm of $M$, $C$ and $K$ vary widely, \cite{Tisseur:2000:BEC} recommends to solve the scaled problem, defined as
\begin{equation}
(\mu^2M_\alpha+\mu C_\alpha+K)x=0,\label{eq:scaled}
\end{equation}
with $\mu=\lambda/\alpha$, $M_\alpha=\alpha^2M$ and $C_\alpha=\alpha C$, where $\alpha$ is a scaling factor. Ideally, $\alpha$ should be chosen in such a way that the norms of $M_\alpha$, $C_\alpha$ and $K$ have similar magnitude. A tentative value would be $\alpha=\sqrt{\frac{\|K\|_\infty}{\|M\|_\infty}}$.

In the general case of polynomials of arbitrary degree, a similar scheme is also possible, but it is not clear how to choose $\alpha$ to achieve the same goal. \cite{Betcke:2008:OSG} proposes such a scaling scheme as well as more general diagonal scalings $D_\ell P(\lambda)D_r$. In \slepc, we provide these types of scalings, whose settings can be tuned with
	\findex{PEPSetScale}
	\begin{Verbatim}[fontsize=\small]
	PEPSetScale(PEP pep,PEPScale scale,PetscReal alpha,Vec Dl,Vec Dr,
                    PetscInt its,PetscReal w);
	\end{Verbatim}
See the manual page for details and the description in \citep{Campos:2016:PKS}.

\paragraph{Extraction.}

Some of the eigensolvers provided in the \texttt{PEP} package are based on solving the linearized eigenproblem of Eq.\ \ref{eq:firstcompfull}. From the eigenvector $y$ of the linearization, it is possible to extract the eigenvector $x$ of the polynomial eigenproblem. The most straightforward way is to take the first block of $y$, but there are other, more elaborate extraction strategies. For instance, one may compute the norm of the residual (\ref{eq:respol}) for every block of $y$, and take the one that gives the smallest residual. The different extraction techniques may be selected with
	\findex{PEPSetExtract}
	\begin{Verbatim}[fontsize=\small]
	PEPSetExtract(PEP pep,PEPExtract extract);
	\end{Verbatim}
For additional information, see \citep{Campos:2016:PKS}.

\paragraph{Controlling and Monitoring Convergence.}

As in the case of \ident{EPS}, in \ident{PEP} the number of iterations carried out by the solver can be determined with \ident{PEPGetIterationNumber}, and the tolerance and maximum number of iterations can be set with \ident{PEPSetTolerances}. Also, convergence can be monitored with command-line keys \Verb!-pep_monitor!, \Verb!-pep_monitor_all!, \Verb!-pep_monitor_conv!, \Verb!-pep_monitor_lg!, or \Verb!-pep_monitor_lg_all!. See \S\ref{sec:monitor} for additional details.

\paragraph{Viewing the Solution.}

Likewise to linear eigensolvers, there is support for various kinds of viewers for the solution. One can for instance use \Verb!-pep_view_values!, \Verb!-pep_view_vectors!, \Verb!-pep_error_relative!, or \Verb!-pep_converged_reason!. See description in \S\ref{sec:epsviewers}.

\subsection{\label{sec:refine}Iterative Refinement}

As mentioned above, scaling can sometimes improve the accuracy of the computed solution considerably, in the case that the coefficient matrices $A_i$ are very different in norm. Still, even when the matrix norms are well balanced the accuracy can sometimes be unacceptably low. The reason is that methods based on linearization are not always backward stable, that is, even if the computation of the eigenpairs of the linearization is done in a stable way, there is no guarantee that the extracted polynomial eigenpairs satisfy the given tolerance.

If good accuracy is required, one possibility is to perform a few steps of iterative refinement on the solution computed by the polynomial eigensolver algorithm. Iterative refinement can be seen as the Newton method applied to a set of nonlinear equations related to the polynomial eigenvalue problem \citep{Betcke:2011:PER}. It is well known that global convergence of Newton's iteration is guaranteed only if the initial guess is close enough to the exact solution, so we still need an eigensolver such as TOAR to compute this initial guess.

Iterative refinement can be very costly (sometimes a single refinement step is more expensive than the whole iteration to compute the initial guess with TOAR), that is why in \slepc it is disabled by default. When the user activates it, the computation of Newton iterations will take place within \ident{PEPSolve} as a final stage (identified as \texttt{PEPRefine} in the \Verb!-log_summary! report).

	\findex{PEPSetRefine}
	\begin{Verbatim}[fontsize=\small]
	PEPSetRefine(PEP pep,PEPRefine refine,PetscInt npart,
                     PetscReal tol,PetscInt its,PEPRefineScheme scheme);
	\end{Verbatim}

There are two types of refinement, identified as \emph{simple} and \emph{multiple}. The first one performs refinement on each eigenpair individually, while the second one considers the computed invariant pair as a whole. This latter approach is more costly but it is expected to be more robust in the presence of multiple eigenvalues.

In \ident{PEPSetRefine}, the argument \texttt{npart} indicates the number of partitions in which the communicator must be split. This can sometimes improve the scalability when refining many eigenpairs.

Additional details can be found in \citep{Campos:2016:PIR}.

\section{\label{sec:qeppep}Upgrading from \texttt{QEP} to \texttt{PEP}}

Users of \slepc versions 3.3 and 3.4 that were employing a \texttt{QEP} solver in their application code will have to adapt it to the new \ident{PEP} object. The process is very simple and basically amounts to replacing \texttt{QEP} by \ident{PEP}. The most important change is when setting the matrices, where the call
	\begin{Verbatim}[fontsize=\small]
        QEPSetOperators( qep, M, C, K );
	\end{Verbatim}
must be replaced with something like this:
	\begin{Verbatim}[fontsize=\small]
        A[0] = K; A[1] = C; A[2] = M;
        PEPSetOperators( pep, 3, A );
	\end{Verbatim}

Other changes in the user interface may have to be taken into account, e.g., \texttt{QEPSetScaleFactor} is now included in \ident{PEPSetScale}.

